{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doctor - HealthCare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-17T19:37:51.494268Z",
     "iopub.status.busy": "2025-01-17T19:37:51.493939Z",
     "iopub.status.idle": "2025-01-17T19:37:51.498615Z",
     "shell.execute_reply": "2025-01-17T19:37:51.497647Z",
     "shell.execute_reply.started": "2025-01-17T19:37:51.494241Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T19:37:52.717638Z",
     "iopub.status.busy": "2025-01-17T19:37:52.717334Z",
     "iopub.status.idle": "2025-01-17T19:37:54.040897Z",
     "shell.execute_reply": "2025-01-17T19:37:54.040120Z",
     "shell.execute_reply.started": "2025-01-17T19:37:52.717607Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         instruction  \\\n",
      "0  If you are a doctor, please answer the medical...   \n",
      "1  If you are a doctor, please answer the medical...   \n",
      "2  If you are a doctor, please answer the medical...   \n",
      "3  If you are a doctor, please answer the medical...   \n",
      "4  If you are a doctor, please answer the medical...   \n",
      "\n",
      "                                               input  \\\n",
      "0  I woke up this morning feeling the whole room ...   \n",
      "1  My baby has been pooing 5-6 times a day for a ...   \n",
      "2  Hello, My husband is taking Oxycodone due to a...   \n",
      "3  lump under left nipple and stomach pain (male)...   \n",
      "4  I have a 5 month old baby who is very congeste...   \n",
      "\n",
      "                                              output  \n",
      "0  Hi, Thank you for posting your query. The most...  \n",
      "1  Hi... Thank you for consulting in Chat Doctor....  \n",
      "2  Hello, and I hope I can help you today.First, ...  \n",
      "3  HI. You have two different problems. The lump ...  \n",
      "4  Thank you for using Chat Doctor. I would sugge...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 112156 entries, 0 to 112155\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   instruction  112156 non-null  object\n",
      " 1   input        112156 non-null  object\n",
      " 2   output       112156 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/input/doctor-healthcare-100k/Doctor-HealthCare-100k.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T19:37:56.299544Z",
     "iopub.status.busy": "2025-01-17T19:37:56.299269Z",
     "iopub.status.idle": "2025-01-17T19:37:56.338397Z",
     "shell.execute_reply": "2025-01-17T19:37:56.337626Z",
     "shell.execute_reply.started": "2025-01-17T19:37:56.299523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Drop Unnecessary Data\n",
    "df = df[['input', 'output']]\n",
    "\n",
    "# Handle Missing Values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T19:37:57.732288Z",
     "iopub.status.busy": "2025-01-17T19:37:57.732013Z",
     "iopub.status.idle": "2025-01-17T19:37:58.648611Z",
     "shell.execute_reply": "2025-01-17T19:37:58.647945Z",
     "shell.execute_reply.started": "2025-01-17T19:37:57.732265Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output'],\n",
      "        num_rows: 100940\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input', 'output'],\n",
      "        num_rows: 11216\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "print(dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T19:38:08.092185Z",
     "iopub.status.busy": "2025-01-17T19:38:08.091897Z",
     "iopub.status.idle": "2025-01-17T19:38:08.176551Z",
     "shell.execute_reply": "2025-01-17T19:38:08.175570Z",
     "shell.execute_reply.started": "2025-01-17T19:38:08.092164Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU for training.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T19:38:30.108523Z",
     "iopub.status.busy": "2025-01-17T19:38:30.108196Z",
     "iopub.status.idle": "2025-01-17T22:04:30.135686Z",
     "shell.execute_reply": "2025-01-17T22:04:30.134957Z",
     "shell.execute_reply.started": "2025-01-17T19:38:30.108494Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d70e9c647e4b248b3e3839e2c80f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b01c376749e04a0697c2b6b0d4d95052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cbeba91ca14b838fe591d3ec877edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4fb2291b9af409d88238af7fe750ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80466efde2f5490aad1c7ebfbbaa292d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831d44e127594af8bdb72aac295cb33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a0efdf33f4446f97298ec41d1e8eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5e4b0abd5248538304c9c174103f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100940 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b73a6daa4644aea916adb822dc88c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11216 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-fea502eb9883>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3154' max='3154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3154/3154 2:22:28, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.818100</td>\n",
       "      <td>1.603507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.620100</td>\n",
       "      <td>1.554957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.599000</td>\n",
       "      <td>1.530887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.575300</td>\n",
       "      <td>1.515001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.554900</td>\n",
       "      <td>1.506438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.547800</td>\n",
       "      <td>1.502588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./fine_tuned_model\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the distilgpt2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# Set padding token to eos_token \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Concatenate input and output for tokenization to ensure consistent padding\n",
    "    concatenated = [f\"{inp} {tokenizer.eos_token} {out}\" for inp, out in zip(examples['input'], examples['output'])]\n",
    "    \n",
    "    # Tokenize the concatenated text\n",
    "    tokenized = tokenizer(\n",
    "        concatenated,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # Use the same input_ids as labels \n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Apply the tokenizer to the dataset\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Use a 90-10 split (train-test)\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Define training arguments \n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    eval_strategy=\"steps\",           \n",
    "    save_strategy=\"steps\",           \n",
    "    save_steps=500,                  \n",
    "    learning_rate=2e-5,              \n",
    "    per_device_train_batch_size=8,  \n",
    "    per_device_eval_batch_size=8,   \n",
    "    num_train_epochs=1,             \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=500,               \n",
    "    load_best_model_at_end=True,     \n",
    "    report_to=\"tensorboard\",         \n",
    "    gradient_accumulation_steps=2,  \n",
    "    fp16=True,  \n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,           \n",
    "    tokenizer=tokenizer,                 \n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model and tokenizer\n",
    "model_save_path = './fine_tuned_model'  \n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T23:08:32.159526Z",
     "iopub.status.busy": "2025-01-17T23:08:32.159239Z",
     "iopub.status.idle": "2025-01-17T23:08:33.220011Z",
     "shell.execute_reply": "2025-01-17T23:08:33.219248Z",
     "shell.execute_reply.started": "2025-01-17T23:08:32.159504Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Im constantly plagued with mouth ulcers and tongue ulcers and feel as if my tongue is swollen, when I lay down my mouth constantly fills with saliva and I constantly have to swallow its  most annoying and I have to try to sleep with my neck up in the air.  My ears are painful too, this is ongoing condition\n",
      "\n",
      "Generated Output: Thanks for your question on Chat Doctor. I can understand your concern. In my opinion, you should consult an ENT specialist and get done clinical examination of your mouth. If you require more of my help in this aspect, I will be happy to help you further. Please do not hesitate to ask in case of any further doubts. Wishing you good health.\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model_save_path = './fine_tuned_model'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_save_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_save_path)\n",
    "\n",
    "# Define pad token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to generate response\n",
    "def generate_response(input_text):\n",
    "    # Add a system prompt to guide the model\n",
    "    system_prompt = (\n",
    "        \"\"\"\n",
    "        You are a professional and empathetic virtual health assistant. Your role is to support users by providing general health-related information, offering helpful suggestions, and guiding them to consult qualified healthcare professionals for personalized advice.\n",
    "\n",
    "        **Guidelines**:\n",
    "        1. Respond with empathy and professionalism, acknowledging user concerns and emotions.\n",
    "        2. Provide general health information that is safe and widely accepted, without diagnosing or recommending specific treatments or medications.\n",
    "        3. Encourage users to seek healthcare professionals for personalized guidance when necessary, but offer general advice where appropriate.\n",
    "        4. Offer non-prescriptive wellness tips, lifestyle recommendations, or general health practices that promote overall well-being.\n",
    "        5. Use simple, clear language, and explain medical terms if requested, but avoid overwhelming the user with too much technical detail.\n",
    "        6. Ensure your responses are reassuring and prioritize the user's overall safety and well-being, while also empowering them to take the next steps.\n",
    "        \n",
    "        **Example**:\n",
    "        User: [User's query]  \n",
    "        Assistant: [Empathetic response with general health information, tips, and encouragement to consult a healthcare professional if needed]\n",
    "        \"\"\"\n",
    "    )\n",
    "    full_input = f\"<|startoftext|>{system_prompt}\\n\\nUser: {input_text}\\nAssistant:<|endoftext|>\"\n",
    "    input_ids = tokenizer(full_input, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=1000,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.pad_token_id  \n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response.split(\"Assistant:\")[-1].strip()\n",
    "\n",
    "# Test on a sample from the test dataset\n",
    "sample_input = dataset[\"test\"][755][\"input\"]\n",
    "print(\"Input:\", sample_input)\n",
    "print(\"\\nGenerated Output:\", generate_response(sample_input))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5997302,
     "sourceId": 9787971,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
